{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import os\n",
    "from ydata_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sjd1111.umd.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>EEG_Analysis</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=EEG_Analysis>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"EEG_Analysis\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EEG_Analysis\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_path = r\"C:\\Users\\sachi\\pyspark_tutorial\\muse_pipeline\\Telepathic-Navigation\\muse_dataset\\Trial_1\"\n",
    "body_parts = [(0,\"Right_hand\"), (1,\"Left_hand\"), (2,\"Right_leg\"), (3,\"Left_leg\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Select relevant columns and drop NA values\n",
    "columns_to_analyze = [\n",
    "    \"TimeStamp\", \"Delta_TP9\", \"Delta_AF7\", \"Delta_AF8\", \"Delta_TP10\",\n",
    "    \"Theta_TP9\", \"Theta_AF7\", \"Theta_AF8\", \"Theta_TP10\",\n",
    "    \"Alpha_TP9\", \"Alpha_AF7\", \"Alpha_AF8\", \"Alpha_TP10\",\n",
    "    \"Beta_TP9\", \"Beta_AF7\", \"Beta_AF8\", \"Beta_TP10\",\n",
    "    \"Gamma_TP9\", \"Gamma_AF7\", \"Gamma_AF8\", \"Gamma_TP10\"]\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [\n",
    "                StructField('TimeStamp', DoubleType(), True),\n",
    "                StructField('Delta_TP9', DoubleType(), True),\n",
    "                StructField('Delta_AF7', DoubleType(), True),\n",
    "                StructField('Delta_AF8', DoubleType(), True),\n",
    "                StructField('Delta_TP10', DoubleType(), True),\n",
    "                StructField('Theta_TP9', DoubleType(), True),\n",
    "                StructField('Theta_AF7', DoubleType(), True),\n",
    "                StructField('Theta_AF8', DoubleType(), True),\n",
    "                StructField('Theta_TP10', DoubleType(), True),\n",
    "                StructField('Alpha_TP9', DoubleType(), True),\n",
    "                StructField('Alpha_AF7', DoubleType(), True),\n",
    "                StructField('Alpha_AF8', DoubleType(), True),\n",
    "                StructField('Alpha_TP10', DoubleType(), True),\n",
    "                StructField('Beta_TP9', DoubleType(), True),\n",
    "                StructField('Beta_AF7', DoubleType(), True),\n",
    "                StructField('Beta_AF8', DoubleType(), True),\n",
    "                StructField('Beta_TP10', DoubleType(), True),\n",
    "                StructField('Gamma_TP9', DoubleType(), True),\n",
    "                StructField('Gamma_AF7', DoubleType(), True),\n",
    "                StructField('Gamma_AF8', DoubleType(), True),\n",
    "                StructField('Gamma_TP10', DoubleType(), True)\n",
    "                # StructField('label', IntegerType(), True)\n",
    "            ]\n",
    "\n",
    "final_struct = StructType(fields=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\"\"\"\n",
    "Load data from a folder containing multiple CSV files.\n",
    "\"\"\"\n",
    "def load_data(index, folder_path, part):\n",
    "\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    # Initialize an empty list to store individual DataFrames\n",
    "    dfs = []\n",
    "    # Read each CSV file and create a DataFrame\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = spark.read.csv(file_path, header=True, schema=final_struct)\n",
    "        df.shape = (df.count(), len(df.columns))\n",
    "        print(f\"Loaded {file} of {part} with shape {df.shape}\")\n",
    "        dfs.append(df)\n",
    "\n",
    "    # Union all DataFrames\n",
    "    combined_df = dfs[0]\n",
    "    for df in dfs[1:]:\n",
    "        combined_df = combined_df.union(df)\n",
    "    combined_df = combined_df.withColumn(\"label\", lit(index))\n",
    "    return combined_df\n",
    "\n",
    "# # Load data for each body part\n",
    "# data_dict = {}\n",
    "# for index, part in body_parts:\n",
    "#     folder_path = os.path.join(base_path, part)\n",
    "#     data_dict[part] = load_data(index, folder_path, part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'start_time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_table\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Create a dataset with the desired granularity\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m data_table \u001b[38;5;241m=\u001b[39m create_dataset(start_time, end_time, columns_to_analyze[\u001b[38;5;241m1\u001b[39m:], granularity)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'start_time' is not defined"
     ]
    }
   ],
   "source": [
    "def create_dataset(start_time, end_time, cols, granularity=0.100):\n",
    "    timestamps = np.arange(start_time, end_time, granularity)\n",
    "    pdf = pd.DataFrame(index=timestamps, columns=cols)\n",
    "    pdf.reset_index(inplace=True)\n",
    "    pdf.columns = ['TimeStamp'] + cols\n",
    "    data_table = spark.createDataFrame(pdf)\n",
    "    for col in cols:\n",
    "        data_table = data_table.withColumn(col, F.lit(None).cast(\"double\"))\n",
    "    return data_table\n",
    "\n",
    "\n",
    "# Create a dataset with the desired granularity\n",
    "data_table = create_dataset(start_time, end_time, columns_to_analyze[1:], granularity)\n",
    "# data_table = data_table.withColumn(\"TimeStamp\", dataset[\"TimeStamp\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (26, 21)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "\n",
    "file_path = r\"C:\\Users\\sachi\\pyspark_tutorial\\muse_pipeline\\Telepathic-Navigation\\muse_dataset\\Trial_2\\Right_hand\\museMonitor_2024-10-11--20-47-37_8793473853970757967.csv\"\n",
    "\n",
    "dataset = pd.read_csv(file_path, header=0).drop(columns=['Elements'])\n",
    "dataset = dataset.dropna()\n",
    "\n",
    "dataset['TimeStamp'] = pd.to_datetime(dataset['TimeStamp'])\n",
    "dataset['TimeStamp'] = ((dataset['TimeStamp'] - dataset['TimeStamp'].min()).dt.total_seconds().round(3)* 1000)\n",
    "# dataset['TimeStamp'] = (dataset['TimeStamp'].astype(float))\n",
    "dataset = dataset[columns_to_analyze]\n",
    "\n",
    "dataset = spark.createDataFrame(dataset, schema=final_struct)\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "# from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "\n",
    "granularity = 100 # Change this to your desired granularity in milliseconds\n",
    "\n",
    "start_time = dataset.select('TimeStamp').collect()[0][0]\n",
    "end_time = dataset.select('TimeStamp').collect()[-1][0]\n",
    "\n",
    "# Create a window\n",
    "window_spec = Window.orderBy('TimeStamp').rowsBetween(0, granularity)\n",
    "\n",
    "# Bucket the data by the time interval (granularity)\n",
    "dataset = dataset.withColumn('TimeBucket', (F.col('TimeStamp') / granularity).cast('int') * granularity)\n",
    "\n",
    "# Group by the buckets and calculate the average for each bucket\n",
    "aggregations = {col: 'avg' for col in columns_to_analyze}\n",
    "dataset = dataset.groupBy('TimeBucket').agg(aggregations)\n",
    "\n",
    "# Rename the columns to remove \"avg()\"\n",
    "for col in columns_to_analyze[1:]:\n",
    "    dataset = dataset.withColumnRenamed(f'avg({col})', col)\n",
    "\n",
    "# Drop the TimeBucket column\n",
    "dataset = dataset.drop('TimeBucket')\n",
    "# Show the resulting dataset\n",
    "# dataset.show()\n",
    "\n",
    "dataset.shape = (dataset.count(), len(dataset.columns))\n",
    "\n",
    "print(f\"Dataset shape: {dataset.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(dataset.count())\n",
    "step = 0.100\n",
    "values = [(i * step,) for i in range(dataset.count())]\n",
    "print(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Create a Spark DataFrame with the new TimeStamp values\n",
    "time_df = spark.createDataFrame([(val,) for val in values], ['TimeStamp'])\n",
    "\n",
    "time_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dataset = dataset.withColumn('row_id', F.monotonically_increasing_id())\n",
    "\n",
    "time_df = time_df.withColumn('row_id', F.monotonically_increasing_id())\n",
    "\n",
    "dataset.count()\n",
    "time_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|           Theta_AF7|          Alpha_AF8|           Delta_AF8|    avg(TimeStamp)|           Gamma_AF7|         Delta_TP10|          Gamma_AF8|          Delta_AF7|         Gamma_TP10|           Beta_AF8|           Beta_TP9|         Delta_TP9|           Theta_AF8|          Gamma_TP9|          Beta_TP10|            Beta_AF7|         Theta_TP10|          Alpha_AF7|         Alpha_TP10|           Theta_TP9|          Alpha_TP9|          TimeStamp|\n",
      "+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "|  0.8463268617647064|0.15179992000000003|  0.8114271123529412|131.02941176470588|-0.19946715764705886| 1.0138810000000003|0.29965797764705887|  1.095687179411765| 0.1766913099999999|0.47546798911764687| 0.4662601599999996|         0.9393861| 0.37934115617647085|         0.26864722| 0.4625017600000002|-0.17047165999999994| 0.7265609499999998| 0.4339591538235298| 0.7592485499999997|           0.7717858| 0.6575103400000005|                0.0|\n",
      "|  0.8201787437499997| 0.1642336799999999|  0.7551045816666662|233.04166666666666|-0.24012707291666668| 1.0138809999999998|0.29654665104166655| 1.0582608479166675| 0.1766913099999999| 0.4888063162499999| 0.4662601599999998|         0.9393861| 0.34790320416666676|         0.26864722|0.46250175999999993|-0.20157065125000007| 0.7265609499999998|0.43295482000000024| 0.7592485499999997|           0.7717858| 0.6575103400000005|                0.1|\n",
      "|          0.85734576|0.13374299999999997|  0.8413322599999998|18.642857142857142|-0.17757607999999997| 1.0138809999999998| 0.3042162400000001| 1.1067277000000002|         0.17669131|0.45750286999999995|         0.46626016|         0.9393861| 0.40164085999999993|         0.26864722|0.46250176000000004|         -0.14776042| 0.7265609499999998|0.43822715000000007|         0.75924855|           0.7717858|         0.65751034|                0.2|\n",
      "|  0.8032895999999997|         0.16423368|  0.7081641238095239| 360.3809523809524| -0.2639954238095238| 1.0138809999999998|0.28740623476190474| 1.0315573095238095|0.17669130999999996| 0.4902152561904761|         0.46626016|0.9393860999999999| 0.32452886428571437|         0.26864722|0.46250176000000004| -0.2133343314285714| 0.7265609499999999|         0.43295482| 0.7592485500000001|           0.7717858|         0.65751034|0.30000000000000004|\n",
      "|  0.7968742630769232|0.17289645153846156|  0.6623254038461538| 513.5384615384615|         -0.26923746| 1.0138809999999998|0.26483813999999994| 1.0231279769230768|         0.17669131|  0.490933743076923|         0.46626016|         0.9393861|  0.2899406676923078|         0.26864722|0.46250176000000004|-0.20768648307692314| 0.7265609499999998|         0.43295482|         0.75924855|           0.7717858|         0.65751034|                0.4|\n",
      "|  0.7991471088888893| 0.1658135566666667|  0.6845331166666665| 443.3333333333333| -0.2671663866666667| 1.0138810000000005|0.27677491555555567| 1.0258683888888893|0.17669130999999993|0.49064043777777755|0.46626015999999965|         0.9393861| 0.30829907111111093|         0.26864722| 0.4625017600000002|-0.21153478000000006| 0.7265609499999998|0.43295482000000024|         0.75924855|           0.7717858| 0.6575103400000002|                0.5|\n",
      "|  0.7959093634285713| 0.1790432931428572|  0.6522436594285713| 619.3714285714286| -0.2751278845714285| 1.0138810000000003|0.24883442199999997|         1.02198224|0.17669130999999996|0.49213111142857124| 0.4662601599999998|0.9393860999999999|  0.2749358720000001|         0.26864722|0.46250176000000004|-0.20401879657142857| 0.7265609499999999|0.43295482000000013| 0.7592485500000001|           0.7717858|         0.65751034| 0.6000000000000001|\n",
      "|  0.7947530533333335|0.19016283444444446|  0.6497563208333333|            855.75|-0.31332173499999993|           1.013881|0.21792172555555556| 1.0218650611111113|0.17669130999999993| 0.5031260011111112| 0.4662601599999999|0.9393860999999999| 0.25135206805555554|         0.26864722| 0.4625017600000001| -0.2084478052777778| 0.7265609499999998| 0.4329548200000001| 0.7592485500000001|           0.7717858|         0.65751034| 0.7000000000000001|\n",
      "|  0.7957647350000001|0.18188012750000002|        0.6519168175| 711.1666666666666|         -0.28585506| 1.0138809999999998|       0.2337772075| 1.0218619750000002|0.17669131000000002| 0.4948907849999999|         0.46626016|         0.9393861|           0.2651225|         0.26864722|0.46250176000000004|-0.20298547749999998| 0.7265609499999998|         0.43295482|         0.75924855|           0.7717858|         0.65751034|                0.8|\n",
      "|  0.7911922571428571|0.20712999928571427|  0.6452358685714286|             928.5| -0.3361822214285714| 0.9484137709999997|0.21115533500000003| 1.0195313785714282|0.17117104785714285|        0.510033625|         0.46626016|         0.9393861| 0.24237771142857148|         0.26864722| 0.4539085128571429|-0.21537623214285717| 0.6759647145714284|         0.43295482| 0.7321921692857144|           0.7717858|         0.65751034|                0.9|\n",
      "|  0.7456369836956521| 0.3124433656521739|  0.5969481565217389|1041.8260869565217| -0.3929507834782609|0.16546569130434788| 0.2053644830434782| 0.9793325552173913|0.10920843586956519| 0.5274616032608695|  0.461234558695652|0.7654694869565215| 0.20644914934782616|0.26526748913043474|0.34586075434782615|  -0.233002594347826|0.03552311673913045|0.43082102826086954|0.40704104282608694|  0.4031350342391304| 0.5077761217391304|                1.0|\n",
      "|  0.6429640333333333| 0.3432481766666666|  0.4997814499999999|            1126.5|-0.43897478333333334|0.37071231416666667|0.20430024166666672| 0.8849384958333335|0.12477278666666668|         0.54068004|0.45525169999999987|0.5584258999999999|  0.1430445008333334|           0.261244| 0.3488849516666668| -0.2445787291666666|       0.0770537085| 0.4149366241666666| 0.4652783699999999|-0.03573492499999...|          0.3295211|                1.1|\n",
      "|      0.514912864375|0.34221078562499996| 0.39160477187500004|         1238.1875|-0.46070850781250017| 0.5382088825000002|0.21008645281250007| 0.7656891234375003|0.12729337500000001|      0.54671806125| 0.4552516999999999|0.5584258999999998| 0.07496677374999998|0.26124399999999987| 0.3423653931250001| -0.2451491193749999|0.11071690387499998| 0.3756674074999998|0.49811390093749985|-0.03573492499999999|          0.3295211| 1.2000000000000002|\n",
      "|-0.07973490843749997| 0.2835030218749999|   0.018111357915625|         1538.4375| -0.4174626503124998| 0.8392413668749998|0.35863013687500006|0.20730731718749992|    0.1158333509375| 0.5747767968749998|0.45525169999999987|0.5584258999999998|-0.12694265281249997| 0.2612439999999998| 0.2903232137500001|-0.10335250006249999|     0.302708594375|0.14495851343750005|    0.6438550328125|-0.03573492499999999| 0.3295211000000001|                1.3|\n",
      "|  0.2325085997500001| 0.3225491265000003|  0.1892562988999998|            1353.9|-0.45949438275000015| 0.7378270525000002|0.24857147599999996| 0.5042731092499999|0.12478436699999992| 0.5524300474999996| 0.4552517000000001|0.5584258999999997|-0.04187691874999...|0.26124399999999975|0.32011045524999987|-0.21231995350000013|0.18569695125000013|0.21262927574999985| 0.5693435262500001|-0.03573492500000...|0.32952110000000034| 1.4000000000000001|\n",
      "|         -0.04870572|         0.28576016|         0.030653916|           1498.25|         -0.42716596|          0.8325978|            0.33188|         0.24202187|         0.11895193|          0.5663861|          0.4552517|         0.5584259|         -0.11886482|           0.261244|         0.29463157|         -0.12978816|          0.2825548|         0.11784208|         0.63262075|        -0.035734925|          0.3295211|                1.5|\n",
      "|-0.15276687750000006|0.27453776166666655|4.626313583333346...|1654.8333333333333| -0.3800601008333333| 0.8551322666666665| 0.4463465083333335|0.12197289625000003|0.10276642833333338| 0.6077587916666665| 0.4552516999999999|0.5584258999999998|-0.15871052166666663| 0.2612439999999998| 0.2798639799999999|-0.01587836083333333|0.35682276666666674|       0.2400178375| 0.6698065666666669|-0.03573492499999999| 0.3295211000000001|                1.6|\n",
      "|-0.16218238833333332|0.27342635833333334|0.029047991000000006|1771.8333333333333|-0.33390593416666664|          0.8574235|0.49758768333333325|0.10009409875000001|0.09768551999999998| 0.6363539666666668|0.45525169999999987|0.5584258999999999|-0.19266515083333335|           0.261244|0.27826812999999995| 0.04133867041666666|0.36799330000000013|0.28632546666666675| 0.6730239000000001|-0.03573492499999...|          0.3295211| 1.7000000000000002|\n",
      "|-0.14760116055555553|0.28013550777777757| 0.07629566999999998|1842.8333333333333|-0.27281708888888884| 0.8574234999999997| 0.5297516166666667|0.10164687533333339|0.09768552000000007| 0.6601514033333329| 0.4552516999999999|0.5584258999999997|-0.22211142833333342| 0.2612439999999998|0.27826813000000006| 0.08815627936111113|0.36799330000000013| 0.2947958533333334| 0.6730239000000003|        -0.035734925| 0.3295211000000003|                1.8|\n",
      "| 0.21864717791666657|0.36999946916666665| 0.34795122291666675|2160.4166666666665|       -0.0651119075| 0.8725479500000001| 0.6092260166666666| 0.4124443041666666|0.10351689041666667| 0.7122954166666666|0.45525169999999987|0.5584258999999999|-0.00944426124999...|           0.261244|0.27498286541666667| 0.22739478666666665| 0.3800632058333335| 0.4001004883333334| 0.6725837854166666|-0.03573492499999...|          0.3295211| 1.9000000000000001|\n",
      "+--------------------+-------------------+--------------------+------------------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+--------------------+-------------------+-------------------+--------------------+-------------------+-------------------+-------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "window_spec = Window.orderBy(F.lit(1))  # Use F.lit(1) to avoid sorting issues\n",
    "\n",
    "# Calculate the number of rows in the dataset\n",
    "num_rows = dataset.count()\n",
    "\n",
    "# Generate the desired timestamp values with step 0.100\n",
    "step = 0.100\n",
    "values = [(i * step,) for i in range(num_rows)]\n",
    "\n",
    "time_df = spark.createDataFrame(values, ['TimeStamp'])\n",
    "\n",
    "# Add a consecutive row number to both the original dataset and the time_df\n",
    "dataset = dataset.withColumn('row_id', F.row_number().over(window_spec))\n",
    "time_df = time_df.withColumn('row_id', F.row_number().over(window_spec))\n",
    "\n",
    "reslt = dataset.join(time_df, 'row_id', 'inner').drop('row_id')\n",
    "\n",
    "reslt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
